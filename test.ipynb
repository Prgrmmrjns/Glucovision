{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jwolber/anaconda3/envs/p313/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "import pickle\n",
    "import optuna\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.special import comb\n",
    "import json\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Constants\n",
    "features = ['simple_sugars', 'complex_sugars', 'fats', 'dietary_fibers', 'proteins', 'insulin']\n",
    "meal_features = ['simple_sugars', 'complex_sugars', 'fats', 'dietary_fibers', 'proteins']\n",
    "features_to_remove = ['glucose_next', 'datetime', 'hour']\n",
    "patient = '001'\n",
    "approaches = ['pixtral-large-latest', 'nollm']\n",
    "prediction_horizons = [6, 12]\n",
    "\n",
    "# Optimization parameters\n",
    "n_trials = 50\n",
    "random_seed = 42\n",
    "n_jobs = 6\n",
    "\n",
    "\n",
    "def bezier_curve(points, num=50):\n",
    "    \"\"\"Generate Bezier curve from control points using Bernstein polynomials\"\"\"\n",
    "    n = len(points) - 1  # Degree of curve is n\n",
    "    t = np.linspace(0, 1, num)\n",
    "    curve = np.zeros((num, 2))\n",
    "    \n",
    "    for i, point in enumerate(points):\n",
    "        # Calculate Bernstein polynomial basis\n",
    "        curve += np.outer(comb(n, i) * (t**i) * ((1-t)**(n-i)), point)\n",
    "    \n",
    "    return curve[np.argsort(curve[:, 0])]\n",
    "\n",
    "def get_projected_value(window, prediction_horizon):\n",
    "    x = np.arange(len(window))\n",
    "    coeffs = np.polyfit(x, window, deg=3)\n",
    "    return np.polyval(coeffs, len(window) + prediction_horizon)\n",
    "\n",
    "def get_data(patient, prediction_horizon):\n",
    "    # Load data\n",
    "    glucose_data = pd.read_csv(f\"diabetes_subset_pictures-glucose-food-insulin/{patient}/glucose.csv\")\n",
    "    insulin_data = pd.read_csv(f\"diabetes_subset_pictures-glucose-food-insulin/{patient}/insulin.csv\")\n",
    "    food_data = pd.read_csv(f\"food_data/pixtral-large-latest/{patient}.csv\")\n",
    "\n",
    "    # Process glucose data\n",
    "    glucose_data[\"datetime\"] = pd.to_datetime(glucose_data[\"date\"] + ' ' + glucose_data[\"time\"])\n",
    "    glucose_data = glucose_data.drop(['type', 'comments', 'date', 'time'], axis=1)\n",
    "    glucose_data['glucose'] *= 18.0182\n",
    "    glucose_data['hour'] = glucose_data['datetime'].dt.hour\n",
    "    glucose_data['time'] = glucose_data['hour'] + glucose_data['datetime'].dt.minute / 60\n",
    "\n",
    "    # Process insulin data\n",
    "    insulin_data[\"datetime\"] = pd.to_datetime(insulin_data[\"date\"] + ' ' + insulin_data[\"time\"])\n",
    "    insulin_data['insulin'] = insulin_data['slow_insulin'] + insulin_data['fast_insulin']\n",
    "    insulin_data = insulin_data.drop(['slow_insulin', 'fast_insulin', 'comment', 'date', 'time'], axis=1)\n",
    "\n",
    "    # Process food data\n",
    "    food_data['datetime'] = pd.to_datetime(food_data['datetime'], format='%Y:%m:%d %H:%M:%S')\n",
    "    food_data = food_data[['datetime', 'simple_sugars', 'complex_sugars', 'proteins', 'fats', 'dietary_fibers']]\n",
    "\n",
    "    # Combine data\n",
    "    combined_data = pd.concat([food_data, insulin_data]).sort_values('datetime').reset_index(drop=True)\n",
    "    combined_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Calculate target variables\n",
    "    glucose_data['glucose_next'] = glucose_data['glucose'] - glucose_data['glucose'].shift(-prediction_horizon)\n",
    "    glucose_data['glucose_change'] = glucose_data['glucose'] - glucose_data['glucose'].shift(1)\n",
    "    \n",
    "    window_size = 6\n",
    "    glucose_data['glucose_change_projected'] = glucose_data['glucose_change'].rolling(\n",
    "        window=window_size, min_periods=window_size\n",
    "    ).apply(lambda window: get_projected_value(window, prediction_horizon))\n",
    "    \n",
    "    glucose_data['glucose_projected'] = glucose_data['glucose'].rolling(\n",
    "        window=window_size, min_periods=window_size\n",
    "    ).apply(lambda window: get_projected_value(window, prediction_horizon))\n",
    "    \n",
    "    glucose_data.dropna(subset=['glucose_next'], inplace=True)\n",
    "    return glucose_data, combined_data\n",
    "\n",
    "def add_features(params, features, data, prediction_horizon):\n",
    "    glucose_data, combined_data = data\n",
    "    \n",
    "    # Convert datetime to nanoseconds for efficient vectorized operations\n",
    "    glucose_times = glucose_data['datetime'].values.astype('datetime64[s]').astype(np.int64)\n",
    "    combined_times = combined_data['datetime'].values.astype('datetime64[s]').astype(np.int64)\n",
    "    \n",
    "    # Calculate time difference matrix (in hours)\n",
    "    time_diff_hours = ((glucose_times[:, None] - combined_times[None, :]) / 3600)\n",
    "    \n",
    "    for feature in features:\n",
    "        \n",
    "        # Generate Bezier curve\n",
    "        curve = bezier_curve(np.array(params[feature]).reshape(-1, 2), num=100)\n",
    "        x_curve, y_curve = curve[:, 0], curve[:, 1]\n",
    "        x0 = x_curve[0] # Get the starting x-coordinate from the curve\n",
    "        \n",
    "        # Create weights array\n",
    "        weights = np.zeros_like(time_diff_hours)\n",
    "        \n",
    "        # For each time difference, find the closest point on bezier curve\n",
    "        for i in range(len(glucose_times)):\n",
    "            for j in range(len(combined_times)):\n",
    "                # Use x0 as the lower bound for the time difference\n",
    "                if time_diff_hours[i, j] >= x0 and time_diff_hours[i, j] <= max(x_curve):\n",
    "                    # Find closest x value in curve\n",
    "                    idx = np.abs(x_curve - time_diff_hours[i, j]).argmin()\n",
    "                    weights[i, j] = y_curve[idx]\n",
    "        \n",
    "        # Compute impact and shift by prediction horizon\n",
    "        feature_values = pd.Series(np.dot(weights, combined_data[feature].values))\n",
    "        glucose_data[feature] = feature_values.shift(-prediction_horizon) - feature_values\n",
    "    return glucose_data\n",
    "\n",
    "def optimize_for_patient(patient, prediction_horizon, base_control_points):\n",
    "    \"\"\"Optimize parameters for a single patient\"\"\"\n",
    "    glucose_data, combined_data = get_data(patient, prediction_horizon)\n",
    "    first_days = glucose_data['datetime'].dt.day.unique()[:3]\n",
    "    mask = glucose_data['datetime'].dt.day.isin(first_days)\n",
    "    train_glucose_data = glucose_data[mask].copy()\n",
    "    data = (train_glucose_data, combined_data) # Pass only training glucose data for optimization\n",
    "    \n",
    "    # --- Start Pre-calculation ---\n",
    "    # Pre-calculate normalized target as it's constant across trials for this patient\n",
    "    target = train_glucose_data['glucose_next'].copy()\n",
    "    target_mean = target.mean()\n",
    "    target_std = target.std()\n",
    "    normalized_target = (target - target_mean) / target_std\n",
    "    # --- End Pre-calculation ---\n",
    "    \n",
    "    # Create Optuna optimization study\n",
    "    study = optuna.create_study(study_name=patient, direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=random_seed), storage=f\"sqlite:///optuna_{patient}.db\", load_if_exists=True)\n",
    "    \n",
    "    # Define the objective function for Optuna\n",
    "    def objective(trial):\n",
    "        # Container for parameters and weights\n",
    "        params = {}\n",
    "        weights = {}\n",
    "        \n",
    "        # Generate parameters and weights for each feature\n",
    "        for feature in features:\n",
    "            base_bounds = base_control_points[feature]\n",
    "            feature_params = []\n",
    "            \n",
    "            # First control point (x optimized, y=0)\n",
    "            x0 = trial.suggest_float(f\"{feature}_x0\", 0.0, 1.0) \n",
    "            feature_params.extend([x0, 0.0]) \n",
    "            \n",
    "            # Second control point (x and y optimized)\n",
    "            # Shift the bounds by x0\n",
    "            min_x1 = x0 + base_bounds[0][0]\n",
    "            max_x1 = x0 + base_bounds[0][1]\n",
    "            x1 = trial.suggest_float(f\"{feature}_x1\", min_x1, max_x1)\n",
    "            y1 = trial.suggest_float(f\"{feature}_y1\", 0.1, 1.0)\n",
    "            feature_params.extend([x1, y1])\n",
    "            \n",
    "            # Third control point (x and y optimized)\n",
    "            min_x2 = x1 + 0.1 # Ensure minimum distance from previous point\n",
    "            max_x2 = x0 + base_bounds[1][1] # Shift max bound by x0\n",
    "            x2 = trial.suggest_float(f\"{feature}_x2\", min_x2, max_x2)\n",
    "            y2 = trial.suggest_float(f\"{feature}_y2\", 0.0, 1.0) # y can be lower/zero\n",
    "            feature_params.extend([x2, y2])\n",
    "\n",
    "            # Fourth control point (x optimized, y=0)\n",
    "            min_x3 = x2 + 0.1 # Ensure minimum distance\n",
    "            max_x3 = x0 + base_bounds[2][1] # Shift max bound by x0\n",
    "            x3 = trial.suggest_float(f\"{feature}_x3\", min_x3, max_x3)\n",
    "            feature_params.extend([x3, 0.0])\n",
    "            \n",
    "            params[feature] = feature_params\n",
    "            # --- Start Weight Suggestion ---\n",
    "            weights[feature] = trial.suggest_float(f\"{feature}_weight\", -2.0, 2.0)\n",
    "            # --- End Weight Suggestion ---\n",
    "        \n",
    "        # Process data with current parameter set\n",
    "        # Important: Use the original 'data' tuple which contains the train_glucose_data\n",
    "        glucose_data_copy, combined_data_copy = data[0].copy(), data[1].copy() \n",
    "        df = add_features(params, features, (glucose_data_copy, combined_data_copy), prediction_horizon)\n",
    "\n",
    "        # --- Use Pre-calculated Normalized Target --- \n",
    "        feature_impacts = df[features].copy()\n",
    "\n",
    "        # Normalize features and apply weights (this depends on trial params, so must be inside)\n",
    "        weighted_normalized_sum = pd.Series(np.zeros(len(df)), index=df.index)\n",
    "        for feature in features:\n",
    "            col = feature_impacts[feature]\n",
    "            col_mean = col.mean()\n",
    "            col_std = col.std()\n",
    "            if col_std == 0: \n",
    "                 normalized_col = pd.Series(np.zeros(len(df)), index=df.index)\n",
    "            else:\n",
    "                 normalized_col = (col - col_mean) / col_std\n",
    "            \n",
    "            weighted_normalized_sum += normalized_col * weights[feature]\n",
    "        \n",
    "        # Calculate the absolute Pearson correlation using pre-calculated normalized_target\n",
    "        # Ensure indices align if add_features modified the index (it shouldn't currently)\n",
    "        correlation = normalized_target.corr(weighted_normalized_sum)\n",
    "\n",
    "        return 1.0 - abs(correlation)\n",
    "        # --- End Change ---\n",
    "    \n",
    "    # Run optimization with parallelize flag\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=n_jobs)\n",
    "    \n",
    "    # Get best parameters and weights\n",
    "    best_params = {'bezier_points': {}, 'weights': {}}\n",
    "    for feature in features:\n",
    "        # Retrieve the optimized x0 for this feature\n",
    "        best_x0 = study.best_params[f\"{feature}_x0\"]\n",
    "        feature_params = [best_x0, 0.0] # First point with optimized x0\n",
    "        \n",
    "        # Bezier control points (use the absolute optimized values)\n",
    "        feature_params.append(study.best_params[f\"{feature}_x1\"])\n",
    "        feature_params.append(study.best_params[f\"{feature}_y1\"])\n",
    "        feature_params.append(study.best_params[f\"{feature}_x2\"])\n",
    "        feature_params.append(study.best_params[f\"{feature}_y2\"])\n",
    "        feature_params.append(study.best_params[f\"{feature}_x3\"])\n",
    "        feature_params.append(0.0)  # y3 is fixed at 0 (relative to the control point P3's x-value)\n",
    "        best_params['bezier_points'][feature] = feature_params\n",
    "        \n",
    "        # Weights\n",
    "        best_params['weights'][feature] = study.best_params[f\"{feature}_weight\"]\n",
    "    \n",
    "    print(f\"Completed optimization for patient {patient}, best score (1 - abs(corr)): {study.best_value:.4f}\")\n",
    "    # Optionally print weights\n",
    "    # print(f\"Best weights for patient {patient}: {best_params['weights']}\")\n",
    "    os.remove(f\"optuna_{patient}.db\")\n",
    "        \n",
    "    return patient, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Format: [[min_x1, max_x1], [min_x2, max_x2], [min_x3, max_x3]] - Bounds for x coordinates of the 3 optimized points P1, P2, P3.\n",
    "# y1 bounds: [0.1, 1.0]\n",
    "# y2 bounds: [0.0, 1.0]\n",
    "# P0 is (0,0), P4 is (x3, 0)\n",
    "base_control_points = {\n",
    "    'simple_sugars': [[0.1, 0.8], [0.5, 1.5], [2.0, 4.0]],   # Fast rise, peak ~1h, return 2-4h\n",
    "    'complex_sugars': [[0.5, 1.5], [1.5, 3.0], [4.0, 7.0]],  # Slower rise, peak 1.5-3h, return 4-7h\n",
    "    'proteins': [[1.0, 2.5], [2.5, 5.0], [5.0, 10.0]],       # Slow rise, peak 2.5-5h, return 5-10h\n",
    "    'fats': [[1.5, 3.0], [3.0, 6.0], [6.0, 14.0]],          # Slowest rise, peak 3-6h, return 6-14h\n",
    "    'dietary_fibers': [[1.0, 3.0], [3.0, 6.0], [6.0, 18.0]], # Blunting effect, long duration\n",
    "    'insulin': [[0.1, 0.5], [0.5, 1.5], [1.5, 4.0]],         # Fast action, peak ~1h, return 1.5-4h\n",
    "}\n",
    "\n",
    "# Main evaluation loop\n",
    "df = pd.DataFrame(columns=['Approach', 'Prediction Horizon', 'Patient', 'Day', 'Hour', 'RMSE']) \n",
    "\n",
    "results = optimize_for_patient(patient, 6, base_control_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
